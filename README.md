
#  HR Analytics â€“ Cloud Deployment (Task 1)

##  Overview
The **HR Analytics Cloud Project** demonstrates a modern data-engineering deployment on **Microsoft Azure**.  
It collects job-advertisement data from the **ArbetsfÃ¶rmedlingen (JobTech) API**, processes it automatically, stores it in a local analytical database (**DuckDB**), and visualizes insights in a **Streamlit** dashboard.  
All components run in Docker containers orchestrated on Azure; intermediate data and logs are shared through a mounted volume (`/mnt/data/`).

---

##  Azure Resources Used
| Resource | Type | Purpose |
|-----------|------|----------|
| `dashdash` | App Service | Hosts the live Streamlit dashboard |
| `ASP-rcbigdatadev-8676` | App Service Plan | Compute plan for App Service |
| `dagstercontainer` | Azure Container Instance | Runs the Dagster ETL pipeline daily |
| `stabigdatadev` | Storage Account | Mounted as `/mnt/data/` for shared DuckDB and logs |
| `crbigdatadev7` | Container Registry (ACR) | Holds Docker images for dashboard and pipeline |


---

##  Architecture

```text
ArbetsfÃ¶rmedlingen API
      â”‚
      â–¼
Dagster Pipeline (ACI)
      â”‚
      â–¼
DuckDB Database (/mnt/data/job_ads.duckdb)
      â”‚
      â–¼
Streamlit Dashboard (App Service)
```

**Workflow**

1. **Dagster** (running in ACI) fetches new data from the API each day.  
2. The pipeline transforms and stores the data in a DuckDB file (`/mnt/data/job_ads.duckdb`).  
3. **Streamlit** (running in App Service) reads the same DuckDB file and renders interactive analytics.  
4. All files and logs are persisted in Azure Storage (`/mnt/data/`).

---

##  Local Development

```bash
git clone https://github.com/<your-org>/azure-cloud-datapipeline.git
cd azure-cloud-datapipeline
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -r requirements.txt
streamlit run dashboard/dashboard.py
```

Default local database path â†’ `/mnt/data/job_ads.duckdb`

---

## ðŸ³ Docker Build and Push

```bash
az login
az acr login --name crbigdatadev7

docker compose build dashboard
docker tag dashboard:latest crbigdatadev7.azurecr.io/dashboard:latest
docker push crbigdatadev7.azurecr.io/dashboard:latest
```

---

##  Deployment on Azure

### 1ï¸âƒ£ Streamlit Dashboard (App Service)
- Connected to ACR `crbigdatadev7`  
- Image: `dashboard:latest`  
- Startup command:
  ```bash
  streamlit run dashboard/dashboard.py --server.address=0.0.0.0 --server.port=8501
  ```
- App Settings:
  ```
  DUCKDB_PATH = /mnt/data/job_ads.duckdb
  ```

### 2ï¸âƒ£ Dagster Container Instance
- Image pulled from ACR  
- Mounts `/mnt/data/`  
- Runs daily ETL job that fetches data from the API and updates DuckDB  
- Logs written to `/mnt/data/logs/`

### 3ï¸âƒ£ Azure Storage Account
- Mounted to both containers as `/mnt/data/`  
- Holds the DuckDB database, logs, and temporary ETL outputs  

---

## ðŸ§¾ Environment Variables
Configure these in **App Service â†’ Configuration â†’ Application Settings**  
and in the Dagster ACI environment:

```
DUCKDB_PATH = /mnt/data/job_ads.duckdb
DLT_STORAGE_PATH = /mnt/data
API_URL = https://jobstream.api.jobtechdev.se
```

---

## âœ… Verification
| Component | Expected Result |
|------------|----------------|
| Streamlit App | Loads at https://dashdash.azurewebsites.net |
| Data Pipeline | Dagster logs confirm daily ETL execution |
| Storage Account | `job_ads.duckdb` updates after each run |
| Dashboard KPIs | Display current vacancies and employer insights |


##  Task 2 â€“ Cost Estimation (Azure Deployment â€“ HR Analytics Project)

This section presents the **final cost estimation** for the HR Analytics â€“ Cloud Deployment project, based on the official calculation from **Azure Pricing Calculator** (Sweden Central region, Pay-as-you-go model).

All resources used in the project are included:  
- Streamlit dashboard (App Service)  
- Dagster ETL container (ACI)  
- Docker image storage (ACR)  
- Blob storage for raw and processed data (Storage Account)

---

### ðŸ“Š Estimated Monthly Costs

| Component | Azure Service | Configuration / Usage | Monthly Cost (USD) | Notes |
|------------|----------------|------------------------|--------------------|-------|
| **Streamlit Dashboard** | App Service (Linux, B1 Plan) | 1 instance â€¢ 1 vCPU â€¢ 1.75 GB RAM â€¢ 730 hours/month | **$13.14** | Always-on web app for HR specialists |
| **Dagster Pipeline** | Azure Container Instance | 1 vCPU â€¢ 4 GB RAM â€¢ 108,000 seconds/month (â‰ˆ 1 hour/day) | **$2.01** | Runs ETL & DLT jobs daily |
| **Container Registry** | Azure Container Registry (Basic Tier) | 1 registry â€¢ 3 GB used (out of 10 GB included) | **$5.00** | Stores Docker images for Streamlit + Dagster |
| **Data Storage** | Azure Storage Account (Blob Hot, LRS) | 5 GB capacity â€¢ 10Ã—10k Write â€¢ 10Ã—10k List â€¢ 10Ã—10k Read ops | **$1.14** | Stores raw / processed data + logs |
| **Total Estimated Cost** |  |  | **â‰ˆ $21.29 per month** |  |

---

### âš™ï¸ Breakdown and Technical Context

- **App Service (dashdash)**  
  Hosts the Streamlit dashboard. The Basic (B1) plan ensures 24/7 uptime with 1 vCore + 1.75 GB RAM.  
  This service is the largest cost driver (~62% of total).

- **Azure Container Instance (dagstercontainer)**  
  Executes the daily ETL pipeline. With 1 vCore + 4 GB RAM, runtime 1 hour/day, it costs roughly $2/month.

- **Azure Container Registry (crbigdatadev7)**  
  Maintains two Docker images (Dagster + Streamlit). Using the Basic tier (10 GB included), actual usage is ~3 GB.

- **Azure Storage Account (stabigdatadev)**  
  Keeps job-ad data fetched from JobTech API, transformed datasets, and log files.  
  Using 5 GB Hot tier with moderate read/write operations results in â‰ˆ $1.14/month.

---

### ðŸ”§ Assumptions

| Parameter | Value / Description |
|------------|---------------------|
| **Region** | Sweden Central |
| **Pricing model** | Pay-as-you-go |
| **Pipeline schedule** | Daily (1 hour runtime per day) |
| **Dashboard uptime** | Always on |
| **Storage usage** | â‰ˆ 5 GB Hot LRS |
| **Registry usage** | â‰ˆ 3 GB within 10 GB Basic tier |
| **Database** | DuckDB embedded (local compute included in ACI runtime) |
| **Cosmos DB** | Excluded (test only, no cost in final estimate) |

---

### ðŸ“ˆ Optimization Opportunities

| Category | Optimization | Potential Saving |
|-----------|---------------|------------------|
| **App Service Plan** | Switch to **Free (F1)** or **Shared (D1)** during development | âˆ’ $13 / month |
| **Container Registry** | Use **Docker Hub** (public) instead of private ACR | âˆ’ $5 / month |
| **Storage Tier** | Move to **Cool Tier** for infrequently accessed data | âˆ’ 30â€“40 % of storage cost |
| **ETL Runtime** | Batch multiple jobs to reduce execution time | âˆ’ $1â€“2 / month |

With these optimizations, the monthly cost could drop to **$10â€“12 (â‚¬9â€“11 / â‰ˆ SEK 120â€“135)** while maintaining functionality.

---

### ðŸ§¾ References

- [Azure Pricing Calculator](https://azure.microsoft.com/en-us/pricing/calculator/)  
- [Azure App Service (Linux) Pricing](https://azure.microsoft.com/en-us/pricing/details/app-service/linux/)  
- [Azure Container Instances Pricing](https://azure.microsoft.com/en-us/pricing/details/container-instances/)  
- [Azure Container Registry Pricing](https://azure.microsoft.com/en-us/pricing/details/container-registry/)  
- [Azure Blob Storage Pricing](https://azure.microsoft.com/en-us/pricing/details/storage/blobs/)

---

### âœ… Summary

> **Total Estimated Cloud Cost:** â‰ˆ **$21.29 / month**  
> (â‰ˆ â‚¬19.6  or  SEK 230 per month)  
>  
> This configuration provides a reliable and scalable cloud setup for the HR Analytics data pipeline, enabling daily data updates and a 24/7 dashboard for talent acquisition specialists.
